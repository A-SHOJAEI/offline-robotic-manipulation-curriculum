# Offline Robotic Manipulation Curriculum

A curriculum learning framework combining Conservative Q-Learning (CQL) with epistemic uncertainty-based scheduling for progressive offline reinforcement learning in robotic manipulation tasks.

## Overview

This framework implements curriculum learning for offline RL by automatically sequencing tasks from simple to complex based on policy uncertainty and performance metrics. It uses Conservative Q-Learning (CQL) to handle distribution shift and ensemble-based uncertainty estimation to determine curriculum progression readiness.

Key contributions:
- **Uncertainty-driven curriculum scheduling** using Q-ensemble disagreement to determine stage progression
- **Behavioral cloning warm-start** for stable policy initialization before CQL fine-tuning
- **Conservative Q-Learning with Lagrange auto-tuning** for robust offline policy optimization
- **Modular curriculum pipeline** supporting multiple scheduling strategies

## Installation

```bash
pip install -r requirements.txt
pip install -e .
```

## Quick Start

Train a curriculum-based CQL agent:

```bash
train-curriculum --config configs/default.yaml
```

Evaluate a trained model:

```bash
evaluate-curriculum --checkpoint models/best_model.pt --num-episodes 50
```

## Architecture

### Conservative Q-Learning (CQL)
Prevents Q-value overestimation on out-of-distribution actions by adding a conservative penalty term. Supports Lagrange multiplier auto-tuning for adaptive conservatism.

- **Q-ensemble**: 5 independent Q-networks for epistemic uncertainty estimation
- **Hidden dimensions**: [256, 256, 256] with ReLU activation and 10% dropout
- **CQL alpha**: 5.0 with Lagrange threshold 10.0

### Curriculum Scheduler
Automatically progresses through task stages based on configurable criteria:
- **Uncertainty-based** (default): Epistemic uncertainty from Q-ensemble disagreement (threshold: 0.15)
- **Performance-based**: Normalized return with patience-based progression
- **Fixed schedule**: Predetermined epoch counts per stage

### Behavioral Cloning Warm-Start
Supervised pre-training (20 epochs per stage) to initialize policies before CQL fine-tuning, providing stable starting points and faster convergence.

### Curriculum Stages

| Stage | Task | Environment | Difficulty | Description |
|-------|------|-------------|------------|-------------|
| 1 | Pen Manipulation | `pen-human-v1` | 1 | Dexterous pen rotation |
| 2 | Door Opening | `door-human-v1` | 2 | Manipulator door interaction |
| 3 | Kitchen Tasks | `kitchen-partial-v0` | 3 | Multi-subtask kitchen environment |
| 4 | Ant Navigation | `antmaze-large-diverse-v2` | 4 | Large maze locomotion planning |

## Training Results

> **Note**: The results below were obtained using **synthetic demonstration data** generated by the framework's built-in data generator. D4RL was not available in the training environment. These results validate the training pipeline and curriculum progression mechanics, but do not reflect performance on real D4RL benchmark datasets. For benchmark-comparable results, install D4RL and retrain with real offline datasets.

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Algorithm | CQL with Q-ensemble (5 networks) |
| Hidden dimensions | [256, 256, 256] |
| BC warm-start | 20 epochs per stage |
| CQL epochs per stage | 50 |
| Total epochs | 200 (4 stages x 50) |
| Batch size | 256 |
| Learning rate | 3e-4 (Adam) |
| Scheduler | Cosine with 10-epoch warmup |
| Device | CPU (48-core) |
| Seed | 42 |
| Total training time | ~79 seconds |

### Behavioral Cloning Warm-Start Losses

| Stage | Task | Final BC Loss |
|-------|------|--------------|
| 1 | pen-human | 1.2539 |
| 2 | door-human | 1.2374 |
| 3 | kitchen-partial | 1.1729 |
| 4 | antmaze-large | 1.1818 |

BC losses decreased consistently across warm-start epochs (from ~1.38 to ~1.25 in stage 1), confirming effective imitation learning initialization.

### CQL Training Metrics (End of Each Stage)

| Stage | Task | Bellman Loss | CQL Penalty | Q-value | Total Loss |
|-------|------|-------------|-------------|---------|------------|
| 1 | pen-human | 0.2646 | 2.2359 | 0.1180 | -7.6145 |
| 2 | door-human | 0.2322 | 2.2087 | 0.1263 | -7.6745 |
| 3 | kitchen-partial | 0.2629 | 2.2881 | 0.0385 | -7.5631 |
| 4 | antmaze-large | 0.2531 | 2.2610 | 0.1188 | -7.6005 |

The negative total loss is expected in CQL, as the conservative penalty (encouraging low Q-values on out-of-distribution actions) dominates the objective. Bellman losses stabilized around 0.23-0.26 across all stages, indicating consistent temporal-difference learning.

### Evaluation Results (Per Stage, Every 10 Epochs)

Each evaluation used 10 episodes with deterministic policy rollouts.

**Stage 1 -- Pen Manipulation**

| Eval Point | Avg Return | Std | Success Rate |
|------------|-----------|-----|-------------|
| Epoch 10 | -0.25 | 0.63 | 0% |
| Epoch 20 | 0.07 | 0.63 | 10% |
| Epoch 30 | 0.38 | 0.99 | 0% |
| Epoch 40 | 0.22 | 0.50 | 10% |
| Epoch 50 | -0.28 | 1.08 | 30% |

**Stage 2 -- Door Opening**

| Eval Point | Avg Return | Std | Success Rate |
|------------|-----------|-----|-------------|
| Epoch 60 | 0.00 | 1.57 | 10% |
| Epoch 70 | 0.04 | 1.18 | 10% |
| Epoch 80 | 0.01 | 0.94 | 0% |
| Epoch 90 | -0.42 | 1.18 | 0% |
| Epoch 100 | 0.26 | 0.60 | 0% |

**Stage 3 -- Kitchen Tasks**

| Eval Point | Avg Return | Std | Success Rate |
|------------|-----------|-----|-------------|
| Epoch 110 | 0.00 | 0.96 | 20% |
| Epoch 120 | -0.44 | 0.74 | 10% |
| Epoch 130 | -0.10 | 0.56 | 10% |
| Epoch 140 | -0.17 | 0.91 | 10% |
| Epoch 150 | -0.65 | 1.26 | 0% |

**Stage 4 -- Ant Navigation**

| Eval Point | Avg Return | Std | Success Rate |
|------------|-----------|-----|-------------|
| Epoch 160 | 0.18 | 1.19 | 0% |
| Epoch 170 | -0.02 | 0.83 | 0% |
| Epoch 180 | 0.12 | 0.69 | 10% |
| Epoch 190 | 0.19 | 1.09 | 0% |
| Epoch 200 | -0.39 | 0.77 | 10% |

### Interpretation

The near-zero average returns and low success rates are expected given the use of **synthetic random data** rather than real D4RL demonstration datasets. The synthetic data does not contain meaningful task-solving trajectories, so the policy has no expert behavior to learn from. What the results do validate:

1. **Pipeline correctness**: All 4 curriculum stages completed successfully with proper stage transitions
2. **Training stability**: Bellman losses and CQL penalties remained stable throughout training, with no divergence
3. **Curriculum progression**: The uncertainty-based scheduler advanced through all stages as designed
4. **BC warm-start effectiveness**: Imitation losses decreased during warm-start, and weights transferred correctly to CQL
5. **Checkpoint saving**: Best and final models saved at `models/best_model.pt` and `models/final_model.pt`

To obtain meaningful task performance, retrain with real D4RL datasets:
```bash
pip install git+https://github.com/Farama-Foundation/D4RL.git
train-curriculum --config configs/default.yaml
```

## Configuration

The framework uses YAML configuration files (`configs/default.yaml`). Key parameters:

- **Curriculum stages**: Task sequence with difficulty levels and success thresholds
- **Scheduling mode**: `uncertainty_based`, `performance_based`, or `fixed_schedule`
- **CQL parameters**: `alpha`, `min_q_weight`, `lagrange_threshold`
- **Model architecture**: Hidden dimensions, ensemble size, activation functions

## Project Structure

```
offline-robotic-manipulation-curriculum/
├── configs/
│   └── default.yaml           # Training configuration
├── src/
│   └── offline_robotic_manipulation_curriculum/
│       ├── data/              # Dataset loading and preprocessing
│       ├── models/            # CQL agent, BC agent, ensemble
│       ├── training/          # Curriculum trainer and scheduler
│       ├── evaluation/        # Policy evaluation and metrics
│       ├── utils/             # Config, logging, network utilities
│       └── scripts/           # Training and evaluation entry points
├── tests/                     # Test suite
├── models/                    # Saved model checkpoints
├── outputs/                   # Training results (JSON)
├── checkpoints/               # Epoch checkpoints
├── notebooks/                 # Analysis notebooks
└── configs/                   # YAML configurations
```

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

## Requirements

- Python 3.8+
- PyTorch 2.0+
- Gymnasium 0.28+
- D4RL (optional, for real datasets)

The framework includes synthetic data generation for testing and development without D4RL/MuJoCo dependencies.

## License

MIT License - Copyright (c) 2026 Alireza Shojaei
